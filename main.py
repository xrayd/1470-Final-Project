import tensorflow as tf
from preprocess import DICT_NAME, TRAIN_NAME, TEST_NAME, OUT_FILE, un_encode, one_hot_smile, pad_smile
import h5py
from model import Model
import numpy as np


def train_model(model, data, batch_size=1000):
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    total_loss = 0
    for i in range(0, data.shape[0] - 390000, batch_size):  # loop over all training examples we have
        inputs = data[i:i+batch_size]  # creating a batch of inputs here
        with tf.GradientTape() as tape:
            out, mu, logvar = model.call(inputs)
            loss = model.loss(out, inputs, mu, logvar)
            print("Batch " + str(i) + " loss: " + str(float(loss)))
            total_loss += loss
        gradient = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradient, model.trainable_variables))
    return total_loss


def generate_molecules(model, character_dict, smile):  # this acts as our test function, as specified in devpost
    """
    Takes in a smile string, then outputs a molecule similar to it by sampling from a learned distribution.
    :param model: TRAINED model, pretty self-explanatory
    :param character_dict: dictionary of character in training set
    :param smile: smile string that we want to use as a base molecule
    :return: smile string of similar molecule generated by our trained model
    """
    one_hot = one_hot_smile(pad_smile(smile), character_dict, preprocess=False)
    one_reshape = np.repeat(one_hot, 1000)  # need this to be compatible with linear layers
    reshape = tf.reshape(one_reshape, [1000, 80, 52])
    distribution = model.call(reshape)[0]  # select the first output of linear layers; they're all the same
    # BEGIN SUMMATION TO 1 CORRECTION
    probabilities = list(distribution[0][0])
    prob_sum = np.sum(probabilities[:-1])
    difference = 1 - prob_sum
    probabilities[-1] = difference
    # END SUMMATION TO 1 CORRECTION
    new_smile = ""
    for i in range(len(distribution)):
        sampled_char_idx = np.random.choice(np.arange(len(character_dict)), p=probabilities)  # samples from dist
        new_smile += character_dict[sampled_char_idx].decode('utf-8')
    return new_smile


def main():
    """
    Reads data from chembl22/chembl22.h5, trains model, then tests model!
    :return:
    """
    data = h5py.File(OUT_FILE)
    train = data[TRAIN_NAME][:]
    test = data[TEST_NAME][:]
    char_dict = list(data[DICT_NAME][:])

    print("Making model...")
    molencoder = Model()

    print("Training...")
    total_loss = train_model(molencoder, train)

    print("Generating similar molecule...")
    new_mol = generate_molecules(molencoder, char_dict, "HC(H)=C(H)(H)")
    print("New Molecule: " + new_mol)
    new_mol = generate_molecules(molencoder, char_dict, "CC")
    print("New Molecule: " + new_mol)
    new_mol = generate_molecules(molencoder, char_dict, "CC(C)(C)CC")
    print("New Molecule: " + new_mol)
    new_mol = generate_molecules(molencoder, char_dict, "CC(CC)C")
    print("New Molecule: " + new_mol)


if __name__ == "__main__":
    main()
